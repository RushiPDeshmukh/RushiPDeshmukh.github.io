<li class="project-item  active" data-filter-item data-category="reinforcement learning">
    <div data-project-item>
      <figure class="blog-banner-box">
        <!-- Project GIF -->
        <img src="./assets/images/BehaviourPlanning.gif"
          alt="Self-Play Based Learning for Multi-Robot RL agents playing Soccer" loading="lazy" project-img>
      </figure>

      <div class="blog-content">
        <div class="blog-meta">
          <!-- Project Category -->
          <p class="blog-category" project-category>Reinforcement Learning</p>
          <span class="dot"></span>
          <!-- Project Date -->
          <time datetime="2024-06" project-date>June 2023</time>
        </div>
        <!-- Project Title -->
        <h3 class="h3 blog-item-title" project-title>Self-Play Based Learning for Multi-Robot RL agents playing Soccer</h3>
        <!-- Project Description -->
        <p class="blog-text" project-description hidden>
            The project involved implementing the Unity ML Agents Kit to train soccer-playing agents using the PPO (Proximal Policy Optimization) algorithm. Unity ML Agents Kit provided a versatile platform for simulating soccer environments and training agents through reinforcement learning. The approach utilized self-play strategies, where RL agents were pitted against previous versions of themselves to enhance learning and improve strategic decision-making. Additionally, ELO scores were employed for posthumous credit assignment (POCA), which allowed for rewarding cooperative RL agents based on their individual performances.
        </p>
        <!-- Overlay: Github Link -->
        <div project-github-link hidden>https://github.com/RushiPDeshmukh/Centralized_Motion_Planner_Project
        </div>
        <!-- Overlay: Keywords -->
        <div project-keywords hidden>Unity MLAgents, Self-Play, ELO score, Cooperative Multi-Agent RL (MARL)</div>
        <!-- Overlay: Results Text -->
        <div project-result-text hidden>
            After just 2 days of training, the soccer-playing agents achieved an impressive ELO score of 1300. This rapid advancement demonstrated the effectiveness of the PPO algorithm and the self-play approach in developing high-performing agents. The use of ELO scores for POCA further facilitated the optimization of agent performance by providing a nuanced evaluation of individual contributions and cooperative behaviors.
        </div>
        <!-- Overlay: Videos + Caption -->
        <div videos hidden>
        </div>
      </div>
    </div>
  </li>
